# -*- coding: utf-8 -*-
"""feature.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13-EnCaM41XNkCwPYxcI7fzDNJ1OvLy19
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import feature_column
from tensorflow.keras import layers
from matplotlib import pyplot as plt

pd.options.display.max_rows = 10
pd.options.display.float_format = "{:.1f}".format
tf.keras.backend.set_floatx("float32")
print("imported the models")

# Load the dataset
train_df = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv")
test_df = pd.read_csv("https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv")

# Scale the labels
scale_factor = 1000.0
# Scale the training set's label.
train_df["median_house_value"] /= scale_factor 

# Scale the test set's label
test_df["median_house_value"] /= scale_factor

# Shuffle the examples
train_df = train_df.reindex(np.random.permutation(train_df.index))

train_df.head()

# Create an empty list that will eventually hold all feature columns.
feature_columns = []

# Create a numerical feature column to represent latitude.
latitude = tf.feature_column.numeric_column("latitude")
feature_columns.append(latitude)

# Create a numerical feature column to represent longitude.
longitude = tf.feature_column.numeric_column("longitude")
feature_columns.append(longitude)

# Convert the list of feature columns into a layer that will ultimately become
# part of the model. Understanding layers is not important right now.
fp_feature_layer = layers.DenseFeatures(feature_columns)

def create_model(my_learning_rate, feature_layer):
  model = tf.keras.models.Sequential()
  model.add(feature_layer)
  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))
  model.compile(optimizer = tf.keras.optimizers.RMSprop(lr = my_learning_rate),
                loss = "mean_squared_error",
                metrics = [tf.keras.metrics.RootMeanSquaredError()])
  return model

def train_model(model, dataset, epochs, batch_size, label_name):
  feature = {name:np.array(value) for name,value in dataset.items()}
  label = np.array(feature.pop(label_name))
  history = model.fit(x=feature, y=label, epochs = epochs, batch_size=batch_size, shuffle = True)

  epochs = history.epoch
  hist = pd.DataFrame(history.history)
  rmse = hist["root_mean_squared_error"]
  return epochs, rmse

def plot_the_loss_curve(epochs, rmse):
  plt.figure()
  plt.xlabel("Epoch")
  plt.ylabel("root mean squared error")
  plt.plot(epochs,rmse,label = "loss curve")
  plt.legend()
  plt.ylim([rmse.min()*0.94,rmse.max()*1.05])
  plt.show()

learning_rate = 0.05
epochs = 30
batch_size = 100
label_name = "median_house_value"

#create and compile the models topography
my_model = create_model(learning_rate,fp_feature_layer)

#train the model on the training set
epochs, rmse = train_model(my_model,train_df,epochs, batch_size,label_name)
plot_the_loss_curve(epochs,rmse)

print("\n evaluate the new model against hte test set")
test_features = {name:np.array(value) for name, value in test_df.items()}
test_label = np.array(test_features.pop(label_name))
my_model.evaluate(x = test_features, y = test_label, batch_size = batch_size)

#@title Double-click to view an answer to Task 1.

# No. Representing latitude and longitude as 
# floating-point values does not have much 
# predictive power. For example, neighborhoods at 
# latitude 35 are not 36/35 more valuable 
# (or 35/36 less valuable) than houses at 
# latitude 36.

# Representing `latitude` and `longitude` as 
# floating-point values provides almost no 
# predictive power. We're only using the raw values 
# to establish a baseline for future experiments 
# with better representations.

#representing latitudes ans longitudes in buckets(bins)
resolution_in_degrees = 1.0
#create a new empty list which will eventually hold the generated columns
feature_columns = []
#create  a bucket feature column for latitudes
latitude_as_a_numeric_column = tf.feature_column.numeric_column("latitude")
latitude_boundaries = list(np.arange(int(min(train_df["latitude"])),
                                     int(max(train_df["latitude"])),
                                     resolution_in_degrees ))
latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column,latitude_boundaries)
feature_columns.append(latitude)

longitude_as_a_numeric_column = tf.feature_column.numeric_column("longitude")
longitude_boundaries = list(np.arange(int(min(train_df["longitude"])),
                                      int(max(train_df["longitude"])),
                                      resolution_in_degrees))
longiitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column,longitude_boundaries)
feature_columns.append(longitude)

buckets_feature_layer = layers.DenseFeatures(feature_columns)

# The following variables are the hyperparameters.
learning_rate = 0.04
epochs = 35

# Build the model, this time passing in the buckets_feature_layer.
my_model = create_model(learning_rate, buckets_feature_layer)

# Train the model on the training set.
epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)

plot_the_loss_curve(epochs, rmse)

print("\n: Evaluate the new model against the test set:")
my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)

#representing latitudes and longitudes as a feature cross
resolution_in_degrees = 1.0
feature_columns = []

#create a bucket feature column for latitude
latitude_as_a_numeric_column = tf.feature.column.numeric_column("latitude")
latitude_boundaries = list(np.arange(int(min(train_df["latitude"])),int(max(train_df["latitude"])), resoultion_in_degrees))
latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column,latitude_boundaries)

#create a bucket feature column for longitude
longitude_as_a_numeric_column = tf.feature_column.numeric_column("longitude")
longitude_boundaries = list(np.arange(int(min(train_df["longitude"])),int(max(train_df["longitude"])), resolution_in_degrees))
longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column,longitude_boundaries)

#create a feature cross for latitude and longitude
latitude_x_longitude = tf.feature_column.crossed_column([latitude,longitude], hash_bucket_size = 100)
crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)
feature_columns.append(crossed_feature)
 #convert the list of feature columns that will later be fed into the model
feature_cross_feature_layer = layers.DenseFeatures(feature_columns)

# The following variables are the hyperparameters.
learning_rate = 0.04
epochs = 35

# Build the model, this time passing in the feature_cross_feature_layer: 
my_model = create_model(learning_rate, feature_cross_feature_layer)

# Train the model on the training set.
epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)

plot_the_loss_curve(epochs, rmse)

print("\n: Evaluate the new model against the test set:")
my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)

#Notice that resolution_in_degrees is set to 1.0. Therefore, each cell represents an area of 1.0 degree of latitude by 1.0 degree of longitude, which corresponds to a cell of 110 km by 90 km. This resolution defines a rather large neighborhood.

#Experiment with resolution_in_degrees to answer the following questions:

#What value of resolution_in_degrees produces the best results (lowest loss value)?
#Why does loss increase when the value of resolution_in_degrees drops below a certain value?
#Finally, answer the following question:

#What feature (that does not exist in the California Housing Dataset) would be a better proxy for location than latitude X longitude.